# 机器学习

##### 机器学习认识

- 目标函数

  在算法模型优化的过程中，优化的方向函数--》每次迭代优化的时候都是让这个目标函数的值最小化-》而我们的最优解其实是目标函数最小化时候取的对应的参数值

- 损失函数

  一般情况下和目标函数是同一个。有的时候损失函数表达的意义是指当我们的模型参数给定的时候，预测值和实际值之间的差距值（通过损失函数计算出来的）

- 目标函数的优化方式

  目标函数一般都是凸函数，常用的优化方案：最小二乘法、梯度下降

- 交叉验证

  训练集、测试集；将训练集分成多份，训练多个模型，选择最好的模型。然后在用测试集评估效果。


### 线性回归

用电预测：

- 时间与功率

  将时间转换为年月日时分秒 转换为6个参数

  ax1+bx2=y

  a和b表示的是x1和x2对y的影响。所以一般会对数据做转换，使得x1和x2的数值范围差不多，将标准差做到1。叫数据标准化。

  - 数据分割
  - 数据归一化
  - 模型训练
  - 结果预测
  - 模型评估
  - 模型部署
    - 模型结果输出：使用模型，将其他数据预测完存放在数据库中
    - 模型输出：将模型保存在本地，其他系统使用的时候加载模型，预测结果

- 默认的线性回归是直线，现实中可能有曲线的数据，需要将数据扩展

  - 多项式扩展：将特征与特征之间融合，从而形成新的特征的一个过程，从数学空间上来讲，就是将低维空间的点映射到高维空间中。属于一种特征工程的一种操作
  - 通过多项式扩展后，我们提搞准确率（不是阶数越高越好，有可能过拟合）
  - 过拟合：如果模型在训练集上效果非常好，而在测试集上效果不好；那么这个时候存在过拟合的情况，多项式扩展的时候，如果阶数指定的比较大，有可能导致过拟合。从线性回归模型中来讲，我们可以认为训练出来的模型参数值越大，就表示越存在过拟合的情况。
  - 可以使用Ridge(L2-norm)和LASSO（L1-norm)解决过拟合问题

- L1-norm的有些参数可能为0，所以可以用来做特征选择，为0的参数说明不影响最终的结果。

##### 定义

- 从数据中找出特征矩阵X和目标属性Y之间的线性关系/线性函数
- 多项式扩展：将低纬度空间数据映射到高纬度空间中（a,b)->(a,b,ab,a*a,b*b) 
- 线性回归+多项式扩展：如果数据本身不是线性关系，那么直接使用线性回归模型效果不会太好，会存在欠拟合；数据在低维空间中不是线性关系，但是如果将数据映射到高纬度空间时，数据就有可能变成线性关系，从而就可以使用线性回归；如果映射的维度过高，那么数据就会完全变成线性的，训练出来的模型会非常的契合训练数据，实际上的数据可能和训练数据有差距，从而导致模型在其他数据集上效果不佳，存在过拟合

- 回归算法是一种比较常用的机器学习算法那，用来解释自变量X和因变量Y之间的关系；从机器学习的角度来讲，用于构建一个算法模型（函数）来做属性（X）与标签（Y）之间的映射关系，在算法学习过程中，试图寻找一个函数使得参数之间的关系拟合性最好。
- 输入两个矩阵，参数矩阵+结果矩阵
- 输出模型，可以输出每个自变量的系数， 可以将参数带入，得到预测值。

##### 机器学习调参

在实际工作中，算法原理的作用是面试、加深了解。实际工作中不需要算法推导，算法都已经实现了。

交叉验证：一般是五折交叉验证（将样本数据分成5个部分）

##### 梯度下降法

学习率：学习率过大，每次迭代变化比较大，有可能跳过最优解；过小的话，就太慢了，一般取0.001, 0.01, 0.1

所有的机器学习模型如果可以离线训练好的，那么一般都是离线训练好的，在线部分只是做预测操作。

只要是目标函数是凸函数，就使用梯度下降方法求解，比如普通的线性回归和L2模型

L1 用坐标轴下降法求解



##### 算法模型

线性回归（Linear)、岭回归（Ridge)、LASSO回归、Elastic Net(弹性网络)

正则化：L1-norm, L2-norm



##### 局部加权归回

离预测点越近，权重越大；



##### Logistic 回归

分类算法，而且是二分类，Y取值范围（0,1），监督学习

Y的属性为float时，默认为回归

Y的属性为int时，默认为分类

所以Logistic回归的Y的属性必须是int

输入是X和Y

输出是属于0和1的概率

解释性强

##### Softmax回归

在scikit-learn中和Logistic回归的实现是一样的

不是二分类，而是K分类。不适合多分类，输出的属于每一类别是概率



##### 总结

机器学习只能处理数值类型，如果是对象类型需要转换成数值

线性回归一般用于回归问题，

Logistic和Softmax模型一般用于分类问题，解释性强。

### KNN算法

监督学习

K近邻（K-nearst neighbors)是一种基本的机器学习算法，

KNN算法可以应用于分类应用中，也可以应用在回归应用中。

KNN在分类预测时，一般采用多数表决法；回归预测时，采用平均值法。

##### 算法原理

1. 从训练集合中获取K个离待预测样本距离最近的样本数据；
2. 根据获取得到的K个样本数据来预测当前待预测样本的目标属性值。

##### 三个要素

K值的选择：K值的不同选择，结果可能不同，可以用交叉验证的方式，取得比较合适的K值

距离的度量：距离公式有很多，一般使用欧式距离

决策规则：在分类模型中，主要使用多数表决法或者加权多数表决法；在回归模型中，主要用平均值法或者加权平均值法。

##### 重点，找到最近的K个样本

- 蛮力实现：计算预测样本到所有训练集样本的距离，然后选择最小的K个样本
- KD数（kd_tree): KD树算法中，首先是对训练数据进行建模，构建KD树，然后根据建好的模型来获取近邻样本
- 还有一些从KD_Tree修改后的求解最近邻点的算法，比如：Ball Tree, BBF Tree, MVP Tree等

##### 参数

weights 权重：uniform(等权重)，distance(距离)

n_neighbors: 邻近数目，默认为5

algorithm：计算方式，可选参数：auto, ball_tree, kd_tree, brute,推荐选择kd_tree

leaf_size: 叶子节点的数目：默认30

### 决策树

##### 信息熵

信息熵：在一个系统中，系统越有序，信息熵越低；系统越混乱，信息熵越高。所以信息熵被认为是一个系统有序程度的度量。

##### 条件熵

给定条件X的情况下，随机变量Y的信息熵就叫条件熵



在已知各种情况发生概率的基础上，通过构建决策树来进行分析的一种方式，是一种直观应用概率分析的图解法；

决策树是一种预测模型，代表的是对象属性与对象值之间的映射关系；

决策树是一种树形结构，其中每个内部节点表示一个属性的测试，每个分支表示一个i额测试输出，每个叶子节点代表一中类别

决策树是一种非常常用的有监督的分类算法

重点在决策树的构建 

##### 决策树分类

分类树：分类标签值

回归树：预测连续值，常用算法ID3，C4.5, CART

##### 决策树分类的停止条件

- 当每个节点只有一种类型的时候，停止构建
- 当前节点中记录数小于某个阀值，同时迭代次数达到给定值时，停止构建

##### ID3算法

构建速度块，实现简单。

只适合小规模数据集，需要将数据放到内存中

##### C4.5算法

准确率较高，实现简单

效率较低，只适合小规模数据集，需要将数据放到内存中

##### CART算法

GINI增益作为分割属性选择的标准，可用于分类和回归两类问题。CART构建的是二叉树

##### ID3, C4.5, CART分类树算法总结

ID3和C4.5算法只适合在小规模数据集上使用

当属性值比较多的时候，最好考虑C4.5算法

CART算法是三种算法中最常用的一种决策树构建算法

### 网格交叉验证

主要用于模型开发阶段中啊很粗模型的最优参数的一种方式，内部会用到交叉验证。

对每个参数组合都进行一次K折交叉验证，将K折交叉验证的K个模型的均值作为在这组参数在训练集上的模型整体效果；

GridSearchCV最终认为模型整体效果最优的对应参数是最优参数。

决策树构建过程中，每次选择的划分特征的目的、方向是让数据具有更加明显的区分能力；

每次选择的特征其实是具有比较明显的区分能力的特征；

可以认为这些被选择的特征对于y的取值具有更大的影响；

即选择freture_importance。返回各个特征的权重，选择最大的就可以了。



##### 决策树可视化

依赖graphviz服务

方式一：将模型输出dot文件，使用graphviz的命令将dot文件转换为pdf格式的文件

方式二：使用pydotplus插件生成pdf文件

方式三：直接生成图片

##### 分类树和回归树

- 区别：

  a.分类树中使用信息熵、gini系数、错误率作为树“纯度”的度量指标

- 构建过程

  让每次分裂数据集的时候，让分裂之后的数据集更加的“纯”

- 决策树属性选择方式

  a.基于最优划分的规则进行选择，迭代计算所有特征属性上所有划分方式后的“纯度”

  b.基于随机的划分规则：每次划分的时候，都是先选择一定数目的特征，然后在这部分特征中选择出一个最优的划分特征。

- 决策树的欠拟合和过拟合

  欠拟合是指在训练集上效果就不好，可以通过增加树的深度来缓解决策树欠拟合问题

  过拟合问题可以通过限制树的复杂程度来缓解过拟合问题，随机森林

- 网格交叉验证（GridSearchCV)

  便于找到最优的参数

- 决策树的效果评估



### 集成学习（Ensemble Learning)

集成学习的思想是将若干个学习器（分类器&回归器）组合之后产生一个新学习器。

弱分类器指哪些分类准确率只稍微好于随机猜测的分类器

强学习器是指多个弱学习器组合起来的模型

集成算法的成功在于保证弱分类器的多样性，而且集成不稳定的算法也能够得到一个比较明显的性能提升

常见的集成学习思想：

- Bagging

- Boosting

- Stacking

提升模型效果的一种思想

- 随机森林
- 提升算法
- GBDT(迭代决策树)
- Adaboost

##### 为什么需要集成学习

- 弱分类器间存在一定的差异性，这会导致分类的边界不同，也就是说可能存在错误。那么将多个弱分类器合并后，就可以得到更加合理的边界，减少整体的错误率，实现更好的效果

- 数据集过大，可以分别划分产生不同的数据子集，然后使用数据子集训练不同的分类器，最终再合并成为一个大的分类器
- 数据集过小，又放回的产生不同的数据集，使用不同的分类器，最终合并成为一个大的分类器
- 数据的划分比较复杂，很难使用一个模型描述情况，那么可以训练多个模型，然后再进行模型的融合。
- 对于多个异构的特征集的时候，很难训练一个模型，可以训练多个模型，根据参数，融合不同的模型。

##### Bagging方法

Bagging又叫自举汇聚法，在原始数据集上通过**有放回**的抽样方式，重新选择出S个新数据集来分别训练S个分类器的集成技术，也就是说这些模型的训练数据中允许存在重复数据。重复数据的作用，就是让当前数据集训练的模型有一定的偏差。

Bagging在预测新样本分类的时候，会使用多数投票或者求均值的方式来统计最终的分类结果

Bagging的弱学习器可以是基本的算法模型：Linear、Ridge、Lasso、Logistic、Softmax、ID3、C4.5、CART、SVM、KNN等

注意：Bagging方式是有放回的抽样，并且每个子集的样本数量必须和原始样本数量一致，但是子集中允许存在重复数据。

##### 随机森林（Bagging策略的一种实现）

和决策树之间的区别是从所有属性中随机选择K个属性，选择出最佳分割属性作为节点创建决策树。

m个决策树形成随机森林，通过投票表决属于哪个分类。

###### 随机森林的变种

- Extra Tree
  - RF会随机采用来作为子决策树的训练集，而Extra Tree每个子决策树采用原始数据集训练
  - RF在选择划分特征点的时候，会和传统决策树一样，会基于信息增益，基尼系数等来选择最优特征值；而Extra Tree 会随机的选择一个特征值来划分决策树。
  - Extra Tree因为是随机选择特征值的划分点，会导致决策树的规模一般大于RF所生成的决策树。也就是说Extra Tree模型的方差相对于RF进一步减少。在某些情况下，Extra Tree的泛化能力比RF的强（当发现RF存在过拟合的情况时，可以考虑Extra Tree)
- Totally Random Trees Embedding(TRTE)
  - 是一种非监督的数据转化方式。将低维的数据集映射到高维，从而让映射到高维的数据更好的应用于分类归回模型。

- Isolation Forest
  - 异常点检测算法
  - 在随机采样的过程中，一般只需要少量数据即可；
  - 在构建决策树过程中，算法会随机选择一个特征，并对划分特征随机选择一个划分阀值
  - 一般深度是比较小的
  - 对于异常点的判断，则是将测试样本x拟合到T颗决策树上。计算在每颗树上该样本的叶子节点的深度，平均深度越接近1，是异常点的概率越大。

##### Boosting

提升学习是一种机器学习技术，可以用于回归和分类的问题，他每一步产生弱预测模型，并加权累加到总模型中；如果每一步的弱预测模型的生成都是依据损失函数的特度方式的，那么就成为梯度提升。

- AdaBoost算法

  Adaptive Boost是一种迭代算法。每轮迭代中会在训练集上产生一个新的学习器，然后使用该学习器对所有样本进行预测，以评估每个样本的重要性。算法会为每个样本赋予一个权重，每次用训练好的学习器标注、预测各个样本，如果样本被预测的越正确，则将其权重降低，否认提高样本的权重。权重越高的样本在下一个迭代训练中所占的比重就越大。也就是说越难区分的样本在训练过程中会变得越重要；

  整个迭代过程直接错误率足够小或者达到一定的迭代次数为止。

- GBDT

  也是Boosting算法的一种，要求弱学习器必须是CART模型，而且在模型训练的时候，是要求模型预测的样本损失尽可能的小。

  相对于随机森林、adbboost，训练的模型更加稳定

  弱学习器之间存在关联关系，难以并行训练模型

  由多颗决策树组成，所有数的结果累加起来就是最终结果

  与随机森林的区别：

  - 随机森林使用抽取不同的样本构建不同的子树
  - 迭代决策树在构建子树的时候，使用之前子树构建结果后形成的残差作为作为输入数据构建下一个子树；然后最终预测的时候按照子树构建的顺序进行预测，并将预测结果相加

##### Stacking

是指训练一个模型用于组合其他模型的技术。

首先训练出多个不同的模型，然后再以之前训练的各个模块的输出作为输入来训练新的模型，从而得到一个最终的模型。

### 决策树和集成学习回顾

1. 分类决策树、回归决策树
2. 决策树的构建
   1. 目的：让同一个类别、y取值接近的样本在同一个叶子节点中=》让一个叶子节点中的样本足够的“纯”
   2. “纯”的度量方式：
      1. 信息熵
      2. gini系数
      3. 错误率
      4. MAE
      5. MSE
3. 数据划分的特征选择
   1. 基于划分前“纯”度值和划分后的“纯”度值之间的差距，作为特征属性的选择=》信息增益》选择增益越大的特征属性
   2. 基于划分前“纯”度值和划分后的“纯”度之间的差距，然后相对于划分属性的“纯”度比率，作为特征属性选择的依据=》选择增益率大的特征属性
   3. 数据的划分方式
      1. 对于离散数据，如果是构建多叉树，那么此时一个特征的取值就是一个分支
      2. 对于离散数据，如果是构建二叉树，那么将离散数据转换为“属于该值”和“不属于该值”两个类型
      3. 对于连续数据，在连续数据中找出一个split_point点
4. 决策树的预测
5. 欠拟合和过拟合
   1. 欠拟合：增加数的层次解决这个问题
   2. 过拟合：剪枝（前剪枝和后剪枝）
6. 集成学习
   1. 思路：将多个模型进行融合，使用融合之后的结果作为真实的预测值
   2. bagging: 对数据进行重采样，然后使用重采样的数据进行模型训练。
      1. 代表性算法：随机森林
   3. boosting:对训练数据进行更改（样本权重的更改或者样本数据值的更改）然后使用更改后的数据来训练模型，训练之后对模型进行一个加权的操作
      1. AdaBoost:基于样本的权重进行模型训练，权重越高的样本在模型训练的时候起来的决策性作用越大；样本的权重更新规则：如果一个样本在前面一个模型预测错误，那么当前这个样本的权重就越大；模型权重给定规则：模型预测的越准确，模型的权重越高；本质：每次迭代模型，都对之前错误的样本着重考虑
      2. GBDT：基于每一次迭代模型都是在之前模型预测的结果基础上进行预测，每次产生模型都让误差、偏度变得更小；在每次迭代的时候，使用上一次的训练数据和上一次模型训练之后模型的预测值之间的残差作为当前模型的输入）；GBDT中模型的权重都相等。
      3. Stacking:分为两步
         1. 使用原始数据和算法模型训练多个模型
         2. 使用第一阶段训练的多个模型在训练集上的预测值组成一个特征矩阵X，使用原始数据集中的Y，然后在训练一个模型=》模型的集成，而不是数据的算法集成

### 聚类算法

聚类就是对大量未知标注的数据集，按照数据内容存在的数据特征将数据集划分为多个不同的类别，使类别内的数据比较相似，类别之间的数据相似度比较小；属于无监督学习。

聚类算法的重点在计算样本之间的相似度，也叫样本间的距离

- 距离
  - 欧氏距离
- 夹角余弦相似度

- KD距离（相对熵）一般不用
- 杰卡德相似系数 

给定一个有M个对象的数据集，构建一个具有K个簇的模型，其中k<=M.满足以下条件

1. 每个簇至少包含一个对象
2. 每个对象属于且仅属于一个簇
3. 将满足上述条件的k个簇成为一个合理的聚类划分

#### K-means算法

也称K-平均或者K-j均值，是一种使用广泛的最基础的聚类算法，一般作为掌握聚类算法的第一个算法。

中止条件：迭代次数、簇中心变化率、最小平方误差MSE

- 缺点
  - 如果存在异常点，将导致均值偏差比较严重
  - 类别个数不知，需要我们给，选择不同的初始值可能导致不同的簇划分规则；可以构造多个，然后选择最优的。
  - 初值的几个点的不同也会导致结果的不同
  - 不适合嵌套数据
- 优点
  - 理解容易，聚类效果不错
  - 处理大数据集的时候，该算法可以保证较好的伸缩性和高效性
  - 当簇近似高斯分布时，效果比较好

##### 二分K-means算法

一种弱化初始质心的一种算法。

##### K-means++算法

和K-means算法的区别主要在初始的K个中心点的选择方面，选择K个初始中心点比较远

缺点：要一个一个的计算K个节点，存在性能方面的问题

##### K-means||算法

解决K-means++性能问题，主要思路每次获取K个样本，重复该操作O次，然后再将这些抽样出来的样本聚类出K个点，最后使用这K个点作为K-means算法的初始聚簇中心点。实践证明：5次就搞定了。

#### Canopy 算法（没有实现）

是一种“粗”聚类算法，不用提前确定簇的个数。执行速度较快，但精度较低。

聚餐之间是可能存在重叠的，但是不会存在某个对象不属于任何聚簇的情况

先给定R1，R2两个距离

##### 应用场景

先使用canopy算法进行“粗”聚类得到K个聚类中心点，然后使用K个聚类中心点作为初始中心点，进行“细”聚类

#### Mini Batch K-Means算法（大规模数据集）

是K-Means算法的一种优化，采用小规模的数据子集（每次训练使用的数据集是在训练算法的时候随机抽取的数据子集）减少计算时间，同时试图优化目标函数，可以减少K-Means算法的收敛时间，而且产生的结果只是略差于标准的K-Means算法

#### 层次聚类方法

层次聚类方法对给定的数据集进行层次的分解，直到满足某种条件为止，传统的层次聚类算法主要分为两大类算法

合并点、分裂点的选择不容易、大数据集不太适合

##### 凝聚的层次聚类

采用自底向上的策略。

##### 分裂的层次聚类

采用的自顶向下的策略。类似二分K-Means

##### 层次聚类优化算法

BIRCH(平衡迭代消减聚类法)

### SVM(Support Vecor Machine)

是一个二元分类算法，是对感知器算法模型的一种扩展，SVM算法支持线性分类和非线性分类，也可以将SVM应用在多元分类中，在不考虑集成学习算法，不考虑特定的数据集的时候，在分类算法中SVM效果是最好的，但解释比较难。

##### 非线性可分SVM

定义一个从低纬度特征空间到高维特征空间的映射函数，将原来的低维空间映射到高维空间，变成线性可分的。

##### 核函数（用低维计算来近似高维计算，降低计算难度）

- 线性核函数
- 









