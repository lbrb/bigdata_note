# 机器学习

### 线性回归

用电预测：

- 时间与功率

  将时间转换为年月日时分秒 转换为6个参数

  ax1+bx2=y

  a和b表示的是x1和x2对y的影响。所以一般会对数据做转换，使得x1和x2的数值范围差不多，将标准差做到1。叫数据标准化。

  - 数据分割
  - 数据归一化
  - 模型训练
  - 结果预测
  - 模型评估
  - 模型部署
    - 模型结果输出：使用模型，将其他数据预测完存放在数据库中
    - 模型输出：将模型保存在本地，其他系统使用的时候加载模型，预测结果

- 默认的线性回归是直线，现实中可能有曲线的数据，需要将数据扩展

  - 多项式扩展：将特征与特征之间融合，从而形成新的特征的一个过程，从数学空间上来讲，就是将低维空间的点映射到高维空间中。属于一种特征工程的一种操作
  - 通过多项式扩展后，我们提搞准确率（不是阶数越高越好，有可能过拟合）
  - 过拟合：如果模型在训练集上效果非常好，而在测试集上效果不好；那么这个时候存在过拟合的情况，多项式扩展的时候，如果阶数指定的比较大，有可能导致过拟合。从线性回归模型中来讲，我们可以认为训练出来的模型参数值越大，就表示越存在过拟合的情况。
  - 可以使用Ridge(L2-norm)和LASSO（L1-norm)解决过拟合问题

- L1-norm的有些参数可能为0，所以可以用来做特征选择，为0的参数说明不影响最终的结果。

##### 定义

- 回归算法是一种比较常用的机器学习算法那，用来解释自变量X和因变量Y之间的关系；从机器学习的角度来讲，用于构建一个算法模型（函数）来做属性（X）与标签（Y）之间的映射关系，在算法学习过程中，试图寻找一个函数使得参数之间的关系拟合性最好。
- 输入两个矩阵，参数矩阵+结果矩阵
- 输出模型，可以输出每个自变量的系数， 可以将参数带入，得到预测值。

##### 机器学习调参

在实际工作中，算法原理的作用是面试、加深了解。实际工作中不需要算法推导，算法都已经实现了。

交叉验证：一般是五折交叉验证（将样本数据分成5个部分）

##### 梯度下降法

学习率：学习率过大，每次迭代变化比较大，有可能跳过最优解；过小的话，就太慢了，一般取0.001, 0.01, 0.1

所有的机器学习模型如果可以离线训练好的，那么一般都是离线训练好的，在线部分只是做预测操作。

只要是目标函数是凸函数，就使用梯度下降方法求解，比如普通的线性回归和L2模型

L1 用坐标轴下降法求解



##### 算法模型

线性回归（Linear)、岭回归（Ridge)、LASSO回归、Elastic Net(弹性网络)

正则化：L1-norm, L2-norm



##### 局部加权归回

离预测点越近，权重越大；



### Logistic 回归

分类算法，而且是二分类，Y取值范围（0,1），监督学习

Y的属性为float时，默认为回归

Y的属性为int时，默认为分类

所以Logistic回归的Y的属性必须是int

输入是X和Y

输出是属于0和1的概率

### Softmax回归

在scikit-learn中和Logistic回归的实现是一样的

不是二分类，而是K分类