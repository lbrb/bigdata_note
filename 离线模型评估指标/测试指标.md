# 测试指标

### 算法团队提供离线测试指标

##### 模型维度:

召回准确率, 

召回覆盖率, 

模型离线auc,

离线logloss；

##### 策略维度

内容多样性, 

内容新颖度, 

内容惊喜度, 

内容频控, 

内容时效性,

标题党，

低俗；



AUC：0,75

Logloss: 0.5



### 错误率

错误率指的是在所有测试样例中错分的样例比例。实际上，这样的度量错误掩盖了样例如何被分错的事实。在机器学习中，有一个普遍适用的称为混淆矩阵（confusion matrix)， 他可以帮助人们更好的了解分类中的错误。

### 二分类

- 真阳性：检测有结节，且实际有结节；
- 假阳性：检测有结节，但实际无结节；
- 真阴性：检测无结节，且实际无结节；
- 假阴性：检测无结节，但实际有结节；

##### 准确率（accuracy)

提取出的正确样本数/总样本数

(TP+TN)/(TP+TN+FP+FN)

##### 召回率（Recall) 覆盖率

正确的正例样本数/样本中的正例样本数

TP/(TP+FN)

##### 精确率（Precision)正确率

正确的正例样本数/预测为正例的样本数

TP/(TP+FP)



一般来说呢，鱼与熊掌不可兼得。如果你的模型很贪婪，想要覆盖更多的sample，那么它就更有可能犯错。在这种情况下，你会有很高的recall，但是较低的precision。如果你的模型很保守，只对它很sure的sample作出预测，那么你的precision会很高，但是recall会相对低。



##### F值

为精确率和召回率的调和平均值

Precision * Recall * 2 / (Precision + Recall)

基本上呢，问题就是如果你的两个模型，一个precision特别高，recall特别低，另一个recall特别高，precision特别低的时候，f1-score可能是差不多的，你也不能基于此来作出选择。

### ROC(Receiver Operating Characteristic)

- Sensitivity

  = recall = true positive rate

- specificity

   = 1- false positive rate

| Precision            | P(Y=1\|Y^=1) |
| -------------------- | ------------ |
| Sensitivity = Recall | P(Y^=1\|Y=1) |
| Specificity          | P(Y^=0\|Y=0) |

sencitivity和sepcificity是条件于真实label Y的概率的，条件概率。

无论Y的真实概率是多少，都不会影响sensitivity和specifictiy。也就是说这两个公式不会受到不平衡数据的影响，比较客观。而precision会随着测试集里面的正反比例而变化。





是反映敏感性和特异性连续变量的综合指标，roc曲线上每个点反映着对同一信号刺激的感受性。

##### 横坐标

伪正类率，预测为正但实际为负的样本占所有负例样本的比例，该值越大，预测正类中实际负类越多；

真正类率，预测为正且实际为正的样本占所用正例样的比例，该值越大，预测正类中实际正类越多。



在一个二分类模型中，假设采用逻辑回归分类器，其给出针对每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR)，在平面中得到对应坐标点。随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着真正的负实例，即TPR和FPR会同时增大。阈值最大时，对应坐标点为(0,0)，阈值最小时，对应坐标点(1,1)。

### AUC（Area under curve)

是一个模型评价指标，只能用于二分类模型的评价，对于二分类模型，还有很多其他评价指标，比如：logloss，accuracy，precision。



TPR， FRP 样本一旦确定，分母就已经确定了，三个指标的变化随分子增加单调递增。

precision的分母为预测为真的个数，会随着阈值的变化而变化，因此Precision的变化受TP和FP的综合影响，不单调，变化情况不可预测。

TP、FP的值受样本中好坏客户个数的制约，若样本极不均衡，比如好客户过多，则随Recall的增加，FP