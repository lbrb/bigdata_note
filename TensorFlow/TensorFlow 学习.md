# TensorFlow 学习

##### 有时间的时候看下源码

学习内容：

- TensorFlow
- 传统神经网络（BP神经网络）
- RBF神经网络
- CNN
- RNN
- GAN

是一个采用数据流图，用于**数值计算**的开源软件库。

- Tensor(张量)：N维数组
- Flow(流)：基于数据流图的计算

TensorFlow的边有两种链接关系：

数据依赖：

​	前向传播：张量在数据流图中从前往后流动一遍就完成一个前向传播。

​	后向传播：残差在数据流图中从后往前流动一遍就完成一个后向传播。

控制依赖：



TensorFlow可以认为是一种编程工具，使用TensorFlow来实现具体的业务需求，然后我们使用TensorFlow这个工具箱中的各种工具来实现各种功能。TensorFlow实现基本的数值计算、机器学习、深度学习等，使用TensorFlow必须理解下列概念：

- 使用图来表示计算任务
- 在会话的上下文中执行图
- 使用tensor表示数据
- 通过变量来维护状态
- 使用feed/fetch可以为任意的操作赋值或者从其中获取数据



##### 程序结构

- 构建阶段

  使用TensorFlow提供的API构建这个图

- 执行阶段

  将构建好的执行图在给定的会话中执行，并得到执行结果

##### TensorFlow图

TensorFlow编程的重点是根据业务需求，使用TensorFlow的API将业务转换为执行图（有向无环图）

可以通过Vairable和assign完成变量的定义和更新。

##### 控制依赖

tf.control_dependencies

通过TensorFlow提供的一组函数来处理不完全依赖的情况下的操作排序问题。

##### TensorFlow设备

为了在执行操作的时候，充分利用计算资源，可以明确指定操作在哪个设备上执行。

一般情况下，不需要指定使用CPU还是GPU，TensorFlow会自动检测。模型情况下，只会使用第一个GPU，所以，在实际编程中，经常需要明确给定使用的CPU和GPU。



### TensorFlow可视化

tensorboard

##### tensorflow

- FIFOQueue: 先进先出队列
- RandomShuffleQueue:随机混淆队列

### 常用语法

- placeholder

- Variable

  该类型有assign方法，可以更新值

- constant

- control_dependencies 

### TensorFlow 设备

- /cpu:0 表示使用机器CPU计算
- /gpu:0 表示使用第一个GPU运算
- /gpu:1 表示使用第二个GPU运算
- with tf.device('/gpu:0'):

### 变量作用域

- name_scope



- variable_scope
  - tf.get_variable 如果变量不存在就创建，如果存在就返回

### 分布式

支持多机器、多GPU、多CPU等各种模型的组合运行方法

分布式集群的启动需要手动配置ip, 分布式任务也需要手动指定ip（没有spark好用，spark的任务是自动分配的）

### 激活函数

本质上激活函数加入非线性因素。

- sigmoid
- tanh
- relu
- dropout

### 分类函数

### 如何加速神经网络的训练

加速训练的优化方法基本都基于梯度下降的

- 梯度下降法（BGD，SGD**)
- Adadelta
- Adagrad
- Momentum
- Adam**
- Ftrl
- RMSprop\

### TensorFlow实现机器学习

运行方法：

- 加载数据及定义超参数
- 构建网络
- 训练模型
- 评估模型和预测

### 深度学习

### 神经网络

##### 神经网络架构

- 输入层

  特征

- 隐层

  数量少：浅层神经网络

  数量多：深度神经网络

- 输出层

  分类，哑编码，减少代码复杂度？

##### 激活函数

- S函数 sigmoid， 0~1，导函数范围0~1/4
- 双S函数 tanh，-1~1

##### 最优权重值的学习算法

### 神经网络分类

##### 前馈神经网络

CNN

##### 反馈神经网络（递归神经网络）

RNN

### BP算法

- FP

  根据跟定的w，计算出每个节点的值

- BP

  根据FP得到的结果和实际结果之间的差距，利用梯度优化方法优化参数

  对每个w求导，乘以学习率，变更w的值，不断优化

  网络层级较多时，可能会存在梯度消失的问题.

### RPF 神经网络

只有三层，输入层，中间层，输出层

##### 分为两个阶段

- 无监督学习，从样本数据中选择中心点；可以使用聚类算法，也可以随机给定
- 监督学习，可以使用BP算法计算

### DNN（深度神经网络）

存在两个问题：

梯度消失；梯度爆炸

### CNN卷积神经网络

神经网络层次低的时候，效果不好，还不如svm

神经网络层次高的时候，容易出现梯度消失，梯度爆炸等问题

卷积神经网络的出现为了解决这个问题

传统神经网络无法应用到图片处理中，因为是全连接，参数太多了。、

CNN降低了传统神经网络的复杂性

CNN主要应用图像分类和物品识别等场景

##### 卷积神经网络

- 层次结构
  - 数据输入层 input layer
    - 预处理，原因：数据单位不一样，激活函数是有值域限制的，卷积神经网络一般只要做去均值和归一化
      - 去均值，将输入数据的各个维度中心化到0
      - 归一化，将输入数据的各个维度的幅度归一化到同样的范围
      - PCA/白化
        - PCA  去掉特征与特征之间的相关性
        - 白化 是在PCA的基础上，对转换后的数据每个特征轴上的幅度进行归一化

  - 卷积计算层 CONV layer

    - 一组固定的权重和窗口内数据做矩阵内积后求和的过程叫卷积
    - 每个神经元看做一个filter
    - 窗口滑动，filter对局部数据进行计算
    - 相关概念
      - 步长
      - 深度
      - 填充值 zero-padding
    - 卷积神经网络通过局部感知的方式解决传统神经网络参数过度的问题。
      通过步长，使窗口有重叠，使窗口之间的变化比较自然。
    - 参数共享机制：每个神经元链接数据窗的权重是固定的
    - 滑动窗口重叠

  - ReLU激励层 ReLU Incentive layer

    - 常见的激活函数
      - Sigmoid
        - 优点：取值范围0-1，简单，容易理解
        - 缺点：容易保和和终止梯度传递，输出没有0中心化
      - Tanh
        - 优点：取值范围-1-1，易理解，0中心化
        - 缺点：容易饱和和终止梯度传递
      - ReLU **
        - 优点：相比于Sigmoid和Tanh,提升收敛速度，梯度求解公式简单，不会产生梯度消失或梯度爆炸
        - 缺点：没有边界，可以使用变种ReLU6：min(max(0, x),6)，比较容易出现死神经元
        - ReLU函数是从生物学角度，模型出脑神经元的活动，相对于sigmoid优点
          - 单侧抑制
          - 相对宽阔的兴奋边界
          - 稀疏激活性
          - 更快的收敛速度
      - Leaky ReLU
        - 目的为了解决ReLU激活函数中的死神经元的情况，不过实际效果不好。
      - ELU
        - 对ReLU激活函数的小于0部分，进行指数修正，而不是和Leaky ReLU线性修复
      - Maxout
        - 优点：计算简单，不会出现神经元饱和的情况，不容易出现死神经元的情况
        - 缺点：参数double, 计算量复杂了、
      - 激励层建议
        - CNN尽量不要使用sigmoid,如果要使用，建议只在全连接层使用
        - 首先使用ReLU,迭代速度块，如果效果不佳，考虑使用Leaky ReLU或者Maxout,一般都可以解决了
        - tanh激活函数在某些情况下有比较好的效果，但是应用场景较少

  - 池化层 Pooling Layer

    - 主要功能：通过减少表征的空间尺寸来减少参数量和网络中的计算。池化层在每个特征图上独立操作，使用池化层可以压缩数据和参数的量，减少过拟合。
    - 两种策略
      - 最大池化，一般使用这个，使用窗口中最大值作为特征
      - 平均池化

  - 全连接层 FC layer

    - 全局角度

  - batch Normalization Layer(可能有) 主要是做归一化

    

- 数据处理

- 训练算法

  - 找到最小损失函数的w和b，cnn中常用的是SGD，其实就是一般深度学习中的BP算法，BP算法的核心是链式求导法则

- 优缺点

  - 优点
    - 共享卷积核（共享参数），对高维数据的处理没有压力
    - 无需选择特征属性，只要训练好权重，即可得到特征值
    - 深层次的网络抽取图片信息比较丰富，表达效果好
  - 缺点
    - 需要调参，需要大量样本，训练迭代次数比较多，最好用GPU训练
    - 物理含义不明确，从每层输出中很难看出含义来

- 参数初始化

  - 权重的初始化
    - 一般方式，很小的随机数，服从均值为0，方差（建议：2/n,n为权重数量）的高斯分布，不能初始化为0
  - 偏置项的初始化
    - 一般直接设置为0，在存在ReLU激活函数的网络中，可以考虑设置为一个很小的数

##### 解决过拟合问题

- 正则化

  通过在cost函数上添加一个正则项的方式来降低过拟合问题

- Dropout**

  通过随机删除神经网络中的神经元来解决过拟合问题

##### 经典的结构与训练方式

##### 数据增强

- 水平翻转（旋转）
- 随机裁剪
- 样本不均衡（采用Label shuffle,增加小众类别的图像数据)
- 模糊处理
- 对颜色的数据增强：图像亮度、饱和度、对比度变化
- 训练和测试要协调

### 图片处理库 (python)

- OpenCV
-  PIL
-  matplotlib
-  tensorflow

rgb

hsv :色调，饱和度，亮度

##### 梯度下降

普通梯度下降（使用所有样本）

随机梯度下降（每次使用一个样本）

小批量梯度下降（使用b个样本）

##### 学习率

一般常用的学习率0.000001， 0.0001， 0.003, 0.01, 0.03, 0.1 0.3 1, 3, 10

学习率除了固定值外，也可以使用分布策略，不同阶段使用不同的学习率

了解常用卷积神经网络

- LeNet-5 做手写字符的识别
- AlexNet 彩色图片识别；从这个网络开始，发展的比较快
- ZF NET 基于AlexNet微调
- Google Net 
- VGG**
- ResNet 残差神经网络

### RNN(递归、循环神经网络)

解决输出内容之间关联关系的问题

有明显上下文特征序列

适应场景：

- NLP（语音模型与文本生成）
- 机器翻译
- 语音识别
- 图像描述生成
- 文本相似度计算等

BP神经网络、卷积神经网络复杂度在深度

RNN网络层次很少，不会超过5层，一般是3层结构，复杂度在隐层的递归过程

RNN神经元的输出不仅和当前的输入有关，还有过去的输入有关。因此可以解决输入内容之间的关系问题。

权重表示：

- 输入层-》隐层 U
- 隐层=》隐层 W
- 隐层=》输出层 V

RNN循环次数多了后，前面的信息因为w的不断相乘，权重会特别小。

##### 双向RNN**

预测一个语句中缺失的词语，需要上下文信息

##### LSTM

RNN因为是循环依赖的，无法很好的表示靠前的数据，LSTM通过增加细胞状态的方式，加强重要的维度，减弱不重要的维度

解决RNN对于长期依赖的问题，没法进行解决，通过增加忘记门、信息增加门的方式，将

特别适合解决这类需要长时间依赖的问题

适合自然语言处理

主要有三个门：

- 忘记门
- 信息增加门
- 输出门

##### tensorflow中和rnn相关的api

- tf.nn.rnn_cell 主要定义了RNN的常见的几种细胞cell

### GAN(生成式对抗网络)

是一种无监督学习，解决的问题是如何从训练样本中学习出新样本

应用场景：

组成结构：

- 生成模型

  功能：按照一定的规则（模型学习出来的），产生一批数据，这批数据要非常符合真实数据的特征

  以生成的样本能够通过检测作为收益

- 判别模型

  功能：期望能够找出生成模型产生出来的假样本数据

  以成功识别G模型生成的样本作为收益

