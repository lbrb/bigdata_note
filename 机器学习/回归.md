# 回归

Y是连续的叫回归；

Y是离散的叫分类；

### 数据使用

- 将数据分两两部分，训练+测试数据
- 将数据分成三部分，训练+验证超参+测试数据

可以使用交叉验证，增加多次训练、测试过程

### 线性回归

线性回归模型：y = a0x0 + a1x1 + a2x2

服从高斯分布的损失函数：最小二乘法

##### 参数计算方法

- 参数可以通过公式直接计算出来；

  - 求出损失函数
  - 对损失函数求导
  - 令导数等于0，求出参数

- 梯度下降算法（维度较多时，几百维之上）

  1. 求出损失函数
  2. 求出损失函数的导函数
  3. 初始化参数（拍脑袋）
  4. 沿着负梯度方向迭代，使损失函数越来越小

  - 批量梯度下降算法（BGD）
    - 对最小二乘函数，求导函数；导函数是包含全量样本数据的。
  - 随机梯度下降算法（SGD）
    - 对最小二乘函数，求导函数；导函数是包含全量样本数据的，但每次提供部分数据
    - 有可能跳出局部最小值

##### 防止过拟合的方法：

- L2-norm
- L1-norm 可以做降维

### Logistic回归

分类问题的首选算法，经典做二分类

多分类，多个二分类，或 SoftMax回归



### SoftMax 回归

解决多分类问题



