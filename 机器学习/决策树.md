# 决策树

### 决策树构建过程

构建一个熵下降最快的树，可能会做一个剪枝，防止过拟合。

### 信息熵

- 熵
- 联合熵
- 条件熵
- 互信息

### 决策树的生成算法

- ID3

  选取熵下降最多的特征进行构建决策树

  有些情况下，可能分类太多（比如用序号来划分，一个分支只有一个点），

- C4.5

  信息增益率，解决ID3中的分类过多问题，利用条件熵/熵来判断

- CART

  基尼指数

### 判断标准

- 熵

  信息增益

- 信息增益率

  条件熵/熵

- Gini系数

  是熵在泰勒展开的一阶类似

### 防止过拟合

- 预剪枝

- 后剪枝

- 随机森林

  噪声占比肯定是比较小的，多个树的分类结果做平均后，噪声自然就被过滤掉了

### Bagging与随机森林

##### Bagging

1. 从样本中重采样选出n个样本
2. 在所有属性上，对这n个样本建立分类器（ID3， C4.5， CART，SVM，Logistic回归等）
3. 重复m次，即获得m个分类器
4. 最后根据这m个分类器的投票结果，决定数据属于哪一类。
5. 主要是ID3，C4.5，CART等弱分类器；SVM，Logistic回归等强分类器一般不做Bagging
6. 理论上，每个树只利用了60%的数据，剩余的数据可以用来做测试数据，不需要设置训练数据、测试数据
7. 样本的数量和原始数据不一定要一样，可以设置参数，叫采样率
8. 特征中，也可以不在所有特征中选择最好的特征，可以在80%中的特征中选择，增加随机性

##### 样本不均衡

- 对多的一方做降采样（效果比重采样好些）
  - 先做聚类，然后每个类中取一部分

- 对少的一方做重采样
- 对少的一方数据合成（两个样本取平均值，生成一个新样本）

##### 随机森林作用

- 分类

- 回归

  分类后，y的值是该节点所在的分类中所有y的均值，将决策树的深度设置的深些，就可以用于回归

- 预处理

  - 在相同的叶子节点中，相似性较强；在不同的叶子节点中，相似性较弱；随机森林的场景中，可以计算两个样本落在一个节点中的次数
  - 决策树使用的特征

- 使用随机森林做特征选择

### 实践

- tree.export_graphviz

  决策数模型训练后，可以导出为dot格式的文件,也可以是pdf、png

- 调参，用不同的深度训练决策树，决定准确度最大的深度

