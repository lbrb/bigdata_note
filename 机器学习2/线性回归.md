# 线性回归的原理

### 线性回归是做什么的

找到一个函数$f(x)$ 使得这个函数能够很好的拟合已有数据，然后利用这个函数来预测趋势

![base_liner](https://github.com/lbrb/hasika_ml/blob/master/pic/myplot.png?raw=true)

### 线性回归的原理是什么

##### 似然估计

设我们要找的函数为$f(x) = \theta_0 + \theta_1x_1 + \theta_2x_2...+\theta_nx_n=\theta^Tx$ ，对于其中一个点$(x_1, y_1)$，将x代入函数得到$y^1=\theta x^1$，这里的$y^1$就是我们根据函数预测的值，为了和真实的$y^1$ 区分，预测的$ y^1 $ 我们记做$y^{1hat}$ 。

真实值和预测值之间的差，我们记做$\varepsilon^1 $, $ \varepsilon^1 = y^1-y^{1hat}$，我们把所有的样本都计算一遍得到{$ \varepsilon^1, \varepsilon^2 ... \varepsilon^m$}, m为样本个数。

（注意）这里我们有个假设，假设{$  \varepsilon^1, \varepsilon^2 ... \varepsilon^m $}是独立同分布的，独立是指每个样本不受其他样本的影响，都是独立的；同分布是指{$  \varepsilon^1, \varepsilon^2 ... \varepsilon^m$}都服从同一个分布，这里我们假设服从高斯分布。

高斯分布的概率密度函数我们是学过的：$f(\varepsilon) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(\varepsilon-\mu)^2}{2\sigma^2}}$

我们可以通过调整b的大小，也就是在上图中将函数上下移动，使得{$  \varepsilon^1, \varepsilon^2 ... \varepsilon^m$}这个高斯分布的$\mu$为0，那么概率密度函数可以化简为$f(\varepsilon) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(\varepsilon)^2}{2\sigma^2}}$  知道了概率密度函数，我们就知道了每个$\theta$在这个高斯分布下的概率$f(\varepsilon) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(\varepsilon)^2}{2\sigma^2}}$  

![gauss](https://github.com/lbrb/hasika_ml/blob/master/pic/guass1.png?raw=true)

左侧是服从（0，1）的高斯分布及概率密度函数，右侧两个是服从（0，3）的高斯分布及概率密度函数。

现在我们讨论两个问题：

- 问题1

  假设一组数据服从左侧的高斯分布，我随机从数据中取出一个数，问哪个数被取出的概率最大？

  答案很明显是0。（因为0的概率是0.4，是最大的）

- 问题2

  现在反过来想，假设有一组数据，服从左侧高斯分布或者右侧高斯分布中的一个，也就是说我们不清楚数据服从的是哪个高斯分布。我们还是随机的从数据中取出一个数，我们能不能根据取出的数，来猜测这一组数据服从哪个高斯分布？

  答案，假设我们取出的数据是5，左侧高斯分布5的概率接近0，右侧高斯分布5的概率大概在0.03左右，那么我们可以猜测该数据服从右侧的高斯分布。理由是什么？是不是就是取出的数，在哪个分布中的概率大，哪个分布就更有可能。

上面的问题中，我们只取了一个数，一个数随机性太大。如果我们取足够多的数，然后计算这些数的联合概率，看看这个联合概率在哪个分布中最大，我们就认为这组数据属于这个分布，没毛病吧。

写出联合概率的公式$P(联合概率) = P（\varepsilon^1, \varepsilon^2 ... \varepsilon^m）$ 

别忘了，我们假设数据是独立同分布的，所以$P(联合概率) = P（\varepsilon^1, \varepsilon^2 ... \varepsilon^m）= P(\varepsilon_1)P(\varepsilon_2)...P(\varepsilon_3)=\prod_0^nP(\varepsilon_i)$  

其中$P(\varepsilon) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(\varepsilon)^2}{2\sigma^2}}$  , $P(联合概率)=\prod_0^n\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(\varepsilon)^2}{2\sigma^2}}$ 

已知$\varepsilon = y^1- y^{1hat} =y^1-\theta x^1 $,带入上式 $P(联合概率)=\prod_0^n\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y^1-\theta x^1)^2}{2\sigma^2}}$ ，联合概率就是似然函数 $L(\theta) = \prod_0^n\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y^1-\theta x^1)^2}{2\sigma^2}}$

现在要求似然函数的最大值，怎么求？答：求偏导。相乘的函数求导比较难，我们在等式两边做对数，函数的增减性不变，$LL(\theta) = log\prod_0^n\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y^1-\theta x^1)^2}{2\sigma^2}} = \sum_1^mlog\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y^1-\theta x^1)^2}{2\sigma^2}} = mlog\frac{1}{\sigma\sqrt{2\pi}} - \frac{1}{\sigma^2}*\frac{1}{2}\sum_1^m(y^i-\theta^Tx^i)^2$

目标函数 $J(\theta) = \frac{1}{2}\sum_1^m(y^i-\theta^Tx^i)^2$ 

对目标函数求导 

$\frac {\part}{\part\theta_i} J(\theta) = \sum_{i=1}^m(h_\theta(x^i)-y^i)x^i_j$

有了导数后，我们可以使用梯度下降算法不断迭代求出最优解

$\theta_j = \theta_j + \alpha\sum_{i=1}^{m}(y^i - h_\theta(x^i))x_j^i$

### 自己写一个线性回归

```python
import numpy as np


class HasikaLinearRegression:
    def __init__(self):
        # theta 函数的参数
        self.theta = np.zeros(1)
        # 样本
        self.X = np.zeros(1)
        # 标记
        self.y = np.zeros(1)
        # 学习率
        self.alpha = 0.01
        # 回归系数
        self.coef_ = np.zeros(1)
        # 截距
        self.intercept_ = np.zeros(1)

    def fit(self, X, y):
        x_0 = np.ones(X.shape[0])
        self.X = np.insert(X, 0, values=x_0, axis=1)
        self.theta = np.ones(self.X.shape[1]).reshape(1, -1)
        self.y = y
        print("X: ", X)
        print("y: ", y)
        print("theta: ", self.theta)
        self.loop()
        self._set_intercept()

    # 不断的迭代学习
    def loop(self):
        loop_n = 0
        while loop_n < 1000:
            gradient = self.get_gradient()
            # print("gradient: ", gradient)
            self.theta -= self.alpha * gradient
            print("theta: ", self.theta)
            loop_n += 1

    # 获取梯度
    def get_gradient(self):
        h_theta = np.dot(self.X, self.theta.reshape(-1, 1))
        # print("h_theta: ", h_theta)
        loss = h_theta - self.y.reshape(-1, 1)
        # print("loss: ", loss)
        gradient = np.dot(loss.T, self.X)
        return gradient
	# 设置回归系数及截距
    def _set_intercept(self):
        self.coef_ = self.theta[0][1:]
        self.intercept_ = self.theta[0][0]

```



### 怎么用线性回归

```python
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from lr.base_liner import HasikaLinearRegression
import numpy as np
import matplotlib as mpl

mpl.rcParams['font.sans-serif'] = [u'SimHei']
mpl.rcParams['axes.unicode_minus'] = False

if __name__ == '__main__':
    # 样本
    X = np.array([0.49, 1.01, 3.02, 5.01])
    X = X.reshape(-1, 1)
    y = np.array([0.51, 1.50, 4.01, 4.81])

    # 轮子
    liner = HasikaLinearRegression()
    liner.fit(X, y)

    # scikit-learn 类库
    liner2 = LinearRegression()
    liner2.fit(X, y)

    # 画样本点
    plt.scatter(X, y, c='r', label='样本')
    # 画轮子函数线
    a = np.linspace(0, 10, 100)
    print('轮子 回归系数, 截距')
    print(liner.coef_, liner.intercept_)
    b1 = liner.intercept_ + a * liner.coef_

    plt.plot(a, b1, c='g', label='轮子')
    # 画scikit-learn类库函数线
    print('画scikit-learn类库函数线')
    print(liner2.coef_, liner2.intercept_)
    b2 = liner2.intercept_ + a * liner2.coef_
    plt.plot(a, b2, c='b', label='scikit-learn类库')

    plt.legend(loc='upper left')
    plt.show()

```

